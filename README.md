# Rational Tree Learning Geometry (RTLG)

> **Classifying learning dynamics via the positive cone of $SL(2,\mathbb{Z})$.**

---

## Table of Contents

1. [Overview](#1-overview)
2. [Algebraic Core: The Positive Monoid](#2-algebraic-core-the-positive-monoid)
3. [Enumeration of Positive Rationals](#3-enumeration-of-positive-rationals)
4. [Equivalence of Tree Structures](#4-equivalence-of-tree-structures)
5. [Hyperbolic Geometry Realization](#5-hyperbolic-geometry-realization)
6. [Lie Algebra Structure and Continuous Flow](#6-lie-algebra-structure-and-continuous-flow)
7. [Diophantine Approximation and Depth](#7-diophantine-approximation-and-depth)
8. [Stochastic Extension](#8-stochastic-extension)
9. [The Learning Embedding](#9-the-learning-embedding)
10. [Derived Learning Invariants](#10-derived-learning-invariants)
11. [Conjectured Correspondences in ML](#11-conjectured-correspondences-in-ml)
12. [Canonical Summary](#12-canonical-summary)
13. [Open Questions](#13-open-questions)

---

## 1. Overview

Standard learning theory models stochastic gradient descent as continuous motion through a loss landscape. This captures optimization but does not expose the **arithmetic structure** underlying phase transitions, plateaus, and abrupt generalization events.

RTLG identifies the **signal-to-noise ratio** of a gradient system — a single positive real scalar — as the fundamental dynamical coordinate. By embedding this ratio into the positive cone of $SL(2,\mathbb{Z})$ via continued fraction approximation, every learning trajectory acquires a canonical discrete representation as a **word in two generators**, with continuous drift captured by hyperbolic flow in $SL(2,\mathbb{R})$.

The framework is built on established mathematics. The learning embedding is axiomatic and minimal. Connections to specific ML phenomena are stated as **conjectures**, not derivations.

**Notation.** $\mathbb{Z}$: integers. $\mathbb{Q}_{>0}$: positive rationals. $\mathbb{R}_{>0}$: positive reals. $SL(2,R)$: $2\times 2$ matrices over ring $R$ with determinant 1.

---

## 2. Algebraic Core: The Positive Monoid

### Definition 1 (Generators)

Define the matrices

$$L = \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}, \qquad R = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}.$$

Both lie in $SL(2,\mathbb{Z})$: entries are non-negative integers and $\det L = \det R = 1$.

### Definition 2 (Positive Monoid)

$$\mathcal{M} = \langle L, R \rangle \subset SL(2,\mathbb{Z})$$

is the monoid generated by $L$ and $R$ under matrix multiplication.

### Theorem 1 (Positive Cone Structure)

$$\mathcal{M} = \left\{ \begin{pmatrix} a & b \\ c & d \end{pmatrix} \in SL(2,\mathbb{Z}) \;:\; a,b,c,d \geq 0 \right\}.$$

Every element of $\mathcal{M}$ has a **unique** expression as a word in $\{L, R\}$.

**Proof sketch.**
- *Closure:* Products of matrices with non-negative integer entries and determinant 1 inherit both properties.
- *Containment:* Any $M = \begin{pmatrix}a&b\\c&d\end{pmatrix}$ with $a,b,c,d \geq 0$ and $\det M = 1$ can be reduced by the greedy Euclidean algorithm: if $a > c$, right-multiply by $L^{-1}$; if $c > a$, right-multiply by $R^{-1}$; termination at the identity $I$ yields a unique decomposition.
- *Uniqueness:* The Euclidean algorithm is deterministic and reversible, so the word in $\{L,R\}$ is unique. No non-trivial relations exist among positive words because any cancellation would require $L^{-1}$ or $R^{-1}$, whose entries are not all non-negative. $\square$

---

## 3. Enumeration of Positive Rationals

### Definition 3 (Projective Map)

For $M = \begin{pmatrix}a&b\\c&d\end{pmatrix} \in \mathcal{M}$ with $c > 0$, define

$$\Phi(M) = \frac{a}{c} \in \mathbb{Q}_{>0}.$$

### Theorem 2 (Calkin–Wilf Correspondence)

$\Phi : \mathcal{M} \to \mathbb{Q}_{>0}$ is a bijection.

**Proof.**
- *Well-defined:* Since $\det M = ad - bc = 1$ and all entries are non-negative integers, $\gcd(a,c) = 1$, so $a/c$ is already in lowest terms.
- *Injectivity:* Two matrices in $\mathcal{M}$ with the same top-left-to-bottom-left ratio $a/c$ have the same continued fraction expansion (since $\gcd(a,c)=1$ and the Euclidean algorithm on $(a,c)$ is deterministic), hence the same word in $\{L,R\}$, hence are equal.
- *Surjectivity:* Given any $p/q \in \mathbb{Q}_{>0}$ in lowest terms, the Euclidean algorithm on $(p,q)$ produces a sequence of partial quotients $[a_0; a_1, \ldots, a_k]$. Setting $M = R^{a_0} L^{a_1} R^{a_2} \cdots$ gives $M \in \mathcal{M}$ with $\Phi(M) = p/q$. $\square$

---

## 4. Equivalence of Tree Structures

The following are all equivalent descriptions of $\mathcal{M}$:

### 4.1 Calkin–Wilf Tree

The binary tree with root $1/1$ and child operations

$$\frac{p}{q} \;\longrightarrow\; \left(\frac{p}{p+q},\; \frac{p+q}{q}\right).$$

Each positive rational appears exactly once. The path from root to $p/q$ encodes the word in $\{L,R\}$ reaching $\Phi^{-1}(p/q)$.

### 4.2 Stern–Brocot Tree

The binary tree constructed by **mediant insertion**: given adjacent fractions $a/b$ and $c/d$, insert $(a+c)/(b+d)$. Starting from $0/1$ and $1/0$ as sentinels, every positive rational appears exactly once in magnitude order.

### Theorem 3 (Tree Isomorphism)

The Calkin–Wilf tree and the Stern–Brocot tree are isomorphic as rooted binary trees; they differ only in their ordering of nodes (breadth-first by ancestry vs. in-order by magnitude).

**Proof.** Both correspond to the same continued fraction decomposition of $p/q$ via the Euclidean algorithm. The labeling differs because Calkin–Wilf traces the path from root to leaf, while Stern–Brocot records the in-order rank. $\square$

### 4.3 Farey Adjacency

### Theorem 4 (Farey Condition)

Adjacent fractions in any Farey sequence $F_n$, and adjacent nodes along any edge of the Stern–Brocot tree, satisfy

$$ad - bc = 1.$$

**Proof.** This is the determinant condition for $SL(2,\mathbb{Z})$: the matrix $\begin{pmatrix}a&b\\c&d\end{pmatrix}$ formed by two Stern–Brocot neighbors has determinant 1 by construction of the mediant. $\square$

### 4.4 Continued Fraction Tree

### Theorem 5 (Word–Continued Fraction Correspondence)

Let $p/q = [a_0;\, a_1,\, \ldots,\, a_k]$ be the continued fraction expansion of $p/q$ (all $a_i \geq 1$ for $i \geq 1$, $a_0 \geq 0$). Then the unique matrix $M \in \mathcal{M}$ with $\Phi(M) = p/q$ is

$$M = R^{a_0}\, L^{a_1}\, R^{a_2}\, \cdots$$

**Corollary (Tree Depth).**
The word length of $M$ in generators $\{L,R\}$ equals $\sum_{i=0}^{k} a_i$, the sum of partial quotients.

**Proof.** Induction on the Euclidean algorithm. Each subtraction step $p \mapsto p - q$ (or $q \mapsto q - p$) corresponds to right-multiplying by $R^{-1}$ (or $L^{-1}$). Counting steps gives the partial quotients. $\square$

### 4.5 Canonical Identification

All of the following are equivalent descriptions of the same mathematical object:

| Structure | View |
|---|---|
| Calkin–Wilf tree | Combinatorial / ancestry |
| Stern–Brocot tree | Combinatorial / magnitude |
| Farey adjacency graph | Arithmetic / determinant |
| Continued fraction tree | Arithmetic / Euclidean |
| Positive cone of $SL(2,\mathbb{Z})$ | Algebraic / group |
| Discrete subgroup acting on hyperbolic plane | Geometric |
| Word metric in generators $\{L,R\}$ | Metric |
| Exponentials of nilpotent Lie algebra elements | Differential |

---

## 5. Hyperbolic Geometry Realization

### Theorem 6 (Hyperbolic Action)

$SL(2,\mathbb{R})$ acts on the upper half-plane $\mathbb{H} = \{z \in \mathbb{C} : \text{Im}(z) > 0\}$ by Möbius transformations

$$M \cdot z = \frac{az + b}{cz + d}, \qquad M = \begin{pmatrix}a&b\\c&d\end{pmatrix}.$$

This action preserves the hyperbolic metric $ds^2 = (dx^2 + dy^2)/y^2$.

**Proof.** Standard; see e.g. Beardon (1983). The Jacobian computation shows the metric tensor is preserved since $\det M = 1$. $\square$

**Corollary.** $SL(2,\mathbb{Z}) \subset SL(2,\mathbb{R})$ acts discretely by hyperbolic isometries. The rational numbers $\mathbb{Q} \cup \{\infty\}$ are precisely the **boundary points** (cusps) of the upper half-plane under this action. Tree paths in $\mathcal{M}$ correspond to geodesic rays in $\mathbb{H}$ terminating at rational boundary points.

---

## 6. Lie Algebra Structure and Continuous Flow

### Definition 4 (Standard Basis of $\mathfrak{sl}(2,\mathbb{R})$)

$$H = \begin{pmatrix}1 & 0\\0 & -1\end{pmatrix}, \qquad E = \begin{pmatrix}0 & 1\\0 & 0\end{pmatrix}, \qquad F = \begin{pmatrix}0 & 0\\1 & 0\end{pmatrix}.$$

These satisfy $[H,E]=2E$, $[H,F]=-2F$, $[E,F]=H$.

### Theorem 7 (Nilpotent Exponentials)

$$\exp(E) = R, \qquad \exp(F) = L.$$

**Proof.** Since $E^2 = 0$, the exponential series truncates: $\exp(E) = I + E = \begin{pmatrix}1&1\\0&1\end{pmatrix} = R$. Identically for $F$. $\square$

### Theorem 8 (Hyperbolic Flow)

$$\exp(tH) = \begin{pmatrix}e^t & 0\\0 & e^{-t}\end{pmatrix}.$$

Under the projective map $\Phi$, this induces

$$\Phi(\exp(tH)\cdot M) = e^{2t}\, \Phi(M).$$

Therefore $\log \Phi(M(t))$ evolves **linearly** at rate $2\lambda$ under the flow $\dot{M} = \lambda H \cdot M$.

**Proof.** Direct computation. $\exp(tH)$ is diagonal, so $\Phi\bigl(\exp(tH)\begin{pmatrix}a\\c\end{pmatrix}\bigr) = e^t a\,/\, e^{-t}c = e^{2t}(a/c)$. $\square$

### Theorem 9 (Left-Invariant Flow)

Let $\dot{M}(t) = X\,M(t)$ for $X \in \mathfrak{sl}(2,\mathbb{R})$. Then $M(t) = \exp(tX)\,M(0)$, and the induced ratio $\Phi(M(t))$ evolves by the corresponding Möbius transformation.

---

## 7. Diophantine Approximation and Depth

### Theorem 10 (Convergent Error Bound)

Let $x \in \mathbb{R}_{>0}$ and $p_n/q_n$ be the $n$-th continued fraction convergent of $x$. Then

$$\left| x - \frac{p_n}{q_n} \right| < \frac{1}{q_n^2}.$$

**Proof.** Classical; follows from the three-term recurrence for convergents and the identity $p_n q_{n-1} - p_{n-1} q_n = (-1)^{n-1}$. $\square$

**Corollary (Depth–Precision Relation).** The approximation error of any node at tree depth $d = \sum a_i$ is $O(1/q^2)$ where $q$ grows at least as fast as the Fibonacci sequence. Deeper nodes represent more complex, higher-precision signal-to-noise states.

---

## 8. Stochastic Extension

When gradient statistics evolve stochastically, the ratio $C(t) = S(t)/N(t)$ is a continuous-time positive random process. The natural model is a **multiplicative diffusion on $\mathbb{R}_{>0}$**, lifted to $SL(2,\mathbb{R})$ as a left-invariant SDE:

$$dM_t = M_t\bigl(\lambda H\,dt + \sigma_E\,dW^E_t\,E + \sigma_F\,dW^F_t\,F\bigr),$$

where $W^E_t$, $W^F_t$ are independent standard Brownian motions, $\lambda$ is the drift rate, and $\sigma_E, \sigma_F \geq 0$ are diffusion coefficients.

**Proposition.** By Itô's lemma applied to $\theta(t) = \log C(t)$:

$$d\theta_t = \left(2\lambda - \frac{\sigma_E^2 + \sigma_F^2}{2}\right)dt + \sigma_E\,dW^E_t + \sigma_F\,dW^F_t.$$

This is a one-dimensional Brownian motion with drift, for which first-passage time distributions to any threshold are available in closed form via the reflection principle.

*Note: This SDE is a modeling choice. Its validity for any particular gradient system is an empirical question.*

---

## 9. The Learning Embedding

### Definition 5 (Gradient Statistics)

Let $g_t \in \mathbb{R}^d$ be a stochastic gradient, decomposed as $g_t = s_t + \xi_t$ where $s_t = \mathbb{E}[g_t]$ is the signal and $\xi_t$ is zero-mean noise. Define:

$$S_t := \|s_t\|, \qquad N_t := \sqrt{\mathbb{E}\|\xi_t\|^2}.$$

The **signal-to-noise ratio** is $C(t) = S_t / N_t \in \mathbb{R}_{>0}$ (defined when $N_t > 0$).

### Definition 6 (Rational Embedding)

For any $r \in \mathbb{R}_{>0}$, define its **canonical rational approximation** at precision $n$ as the $n$-th continued fraction convergent $p_n/q_n$ of $r$. By Theorem 10, $|r - p_n/q_n| < 1/q_n^2$.

By Theorem 2, there is a unique $M \in \mathcal{M}$ with $\Phi(M) = p_n/q_n$.

### Axiom 1 (Learning Embedding)

At each time $t$, the learning state is represented by the unique $M_t \in \mathcal{M}$ satisfying

$$\Phi(M_t) = p_t/q_t,$$

where $p_t/q_t$ is the best rational convergent of $C(t) = S_t/N_t$ at a fixed precision level.

*This is an embedding, not an identification. The gradient system does not "become" a matrix group; its scalar ratio is approximated by a rational point in the tree.*

### Axiom 2 (Binary Dominance)

At each discrete step, the sign of the log-ratio change is definite:

$$\Delta\log C_t = \log C_{t+1} - \log C_t \neq 0 \quad \text{(generically)}.$$

This holds almost surely for continuous stochastic processes and is a non-degeneracy condition on the dynamics.

### Theorem 11 (Learning Trajectory Embedding)

Under Axioms 1 and 2, every gradient system induces a unique path in $\mathcal{M}$:

$$M_0 \to M_1 \to M_2 \to \cdots, \qquad M_{t+1} = G_t M_t, \quad G_t \in \{L, R\},$$

where $G_t = R$ if $\Delta\log C_t > 0$ and $G_t = L$ if $\Delta\log C_t < 0$.

**Proof.**
1. By Axiom 1, each $C(t)$ maps to a unique node $M_t \in \mathcal{M}$.
2. By Axiom 2, each step applies either $L$ or $R$ to the current node.
3. Words in $\{L,R\}$ uniquely determine elements of $\mathcal{M}$ (Theorem 1). $\square$

**Remark.** This theorem is definitional given the axioms. Its empirical content lies in whether $C(t)$ is measurable, whether the binary dominance assumption holds approximately, and whether the resulting path correlates with observable learning transitions.

---

## 10. Derived Learning Invariants

From the embedding, three natural invariants are defined:

**Tree Depth $d(t)$** — the word length of $M_t$ in $\{L,R\}$, equal to $\sum_i a_i$ (the sum of partial quotients of $C(t)$). Measures the arithmetic complexity of the current learning state.

**Path Entropy $H(t)$** — the Shannon entropy of the binary string of $L$/$R$ steps up to time $t$:

$$H(t) = -\hat{p}_L \log \hat{p}_L - \hat{p}_R \log \hat{p}_R,$$

where $\hat{p}_L, \hat{p}_R$ are empirical frequencies. $H(t) \approx 0$ indicates persistent signal or noise dominance; $H(t) \approx 1$ indicates instability or oscillation.

**Approximation Residual $\varepsilon(t)$** — the gap $|C(t) - p_t/q_t|$, bounded above by $1/q_t^2$. Tracks how well the current rational approximation captures the true ratio; large $\varepsilon$ indicates the state is between two tree nodes.

---

## 11. Conjectured Correspondences in ML

The following are **proposed research directions**, not proven results. They state how RTLG invariants *might* correspond to known phenomena and what evidence would be required to confirm or refute them.

| Phenomenon | Proposed RTLG Interpretation | What Would Confirm It |
|---|---|---|
| **Grokking** | The path crosses a depth-transition region after an extended low-entropy plateau, corresponding to the model traversing high-depth (complex) tree nodes before reaching a simpler subtree. | $d(t)$ spikes then collapses at the grokking transition; $H(t) \approx 0$ during plateau. |
| **Double Descent** | $C(t)$ oscillates near $1$ (balanced signal/noise), causing the path to revisit the tree root region, prolonging depth and uncertainty. | $C(t) \approx 1$ at the interpolation threshold; $d(t)$ is locally maximal. |
| **Flat Minima** | Low-depth nodes (simple partial quotient sequences) are more stable under perturbation, as they correspond to coarse rational approximations with large basins. | Flat minima correlate with low $d(t)$ and small $q_t$. |
| **Lottery Tickets** | Pruning finds the simplest ancestor node in the tree consistent with the remaining parameters' signal-to-noise structure. | Pruned networks have lower $d(t)$ than original networks at equivalent performance. |
| **Adaptive Optimizers** | Optimizers such as Adam compute per-parameter signal-to-noise ratios, implicitly running independent tree embeddings for each coordinate. | Per-parameter $C_i(t)$ tracked independently follows the embedding theorem. |

These conjectures make falsifiable predictions. Testing them requires measuring $S_t$, $N_t$ during training, which is feasible with standard gradient logging.

---

## 12. Canonical Summary

The most compact statement of RTLG is:

> The space of positive real signal–noise ratios admits a canonical embedding into the positive monoid of $SL(2,\mathbb{Z})$ via continued fraction approximation. This monoid is equivalently described by the Calkin–Wilf tree, the Stern–Brocot tree, Farey adjacency, and continued fraction expansion — all unified as the rational boundary of the hyperbolic plane under the standard $SL(2,\mathbb{R})$ action. Discrete learning steps correspond to multiplication by the generators $L$ and $R$, which are exponentials of nilpotent Lie algebra elements; continuous signal drift corresponds to the diagonal hyperbolic flow generated by $H$.

The framework reduces to a single algebraic object:

$$\boxed{\text{Positive cone of } SL(2,\mathbb{Z})}$$

All combinatorial, arithmetic, geometric, and differential structures are equivalent presentations of this object.

---

## 13. Open Questions

**Irrational and periodic trajectories.** Quadratic irrationals have eventually periodic continued fraction expansions. Do learning systems with periodic gradient statistics (e.g., cyclic learning rate schedules) produce periodic paths in $\mathcal{M}$? Is the period predictable from the schedule?

**$p$-adic interpretation.** The infinite path string $G_0 G_1 G_2 \cdots \in \{L,R\}^\mathbb{N}$ defines an element of the boundary of the tree, analogous to a $2$-adic integer. Can $p$-adic analysis provide useful metrics on learning trajectory space?

**Vector RTLG.** For networks with $d$ parameters, one obtains $d$ independent signal-to-noise ratios $C_i(t)$, each inducing a path in $\mathcal{M}$. The joint state lives in $\mathcal{M}^d$, which is not a tree but a higher-dimensional order complex. What is the right geometric and algebraic structure for this product space?

**Minkowski's Question-Mark Function.** The function $?(x) : [0,\infty) \to [0,1]$ maps rationals with finite continued fraction expansions uniformly to dyadic rationals, and is a strictly increasing homeomorphism that is singular (derivative zero almost everywhere). If $\ell(t) = ?(C(t))$, learning plateaus correspond to traversals through high-depth regions where $?'$ vanishes. This linearized coordinate may offer a more uniform parameterization of progress. The relationship between $\Delta\ell(t)$ and genuine structural learning transitions warrants systematic investigation.

**Empirical validation.** The primary open question is whether the embedding defined by Axioms 1–2 has predictive power beyond structural visualization. This requires controlled experiments measuring $S_t$ and $N_t$ across architectures, datasets, and learning rate schedules, with outcomes compared to tree-depth and path-entropy trajectories.

