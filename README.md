# Rational Tree Learning Geometry (RTLG)

> **Classifying learning dynamics via the positive cone of $SL(2,\mathbb{Z})$.**

---

## Table of Contents

1. [Overview](#1-overview)
2. [Algebraic Core: The Positive Monoid](#2-algebraic-core-the-positive-monoid)
3. [Enumeration of Positive Rationals](#3-enumeration-of-positive-rationals)
4. [Equivalence of Tree Structures](#4-equivalence-of-tree-structures)
5. [Hyperbolic Geometry Realization](#5-hyperbolic-geometry-realization)
6. [Lie Algebra Structure and Continuous Flow](#6-lie-algebra-structure-and-continuous-flow)
7. [Diophantine Approximation and Depth](#7-diophantine-approximation-and-depth)
8. [Stochastic Extension](#8-stochastic-extension)
9. [The Learning Embedding](#9-the-learning-embedding)
10. [Derived Learning Invariants](#10-derived-learning-invariants)
11. [Conjectured Correspondences in ML](#11-conjectured-correspondences-in-ml)
12. [Canonical Summary](#12-canonical-summary)
13. [Open Questions](#13-open-questions)

---

## 1. Overview

Standard learning theory models stochastic gradient descent as continuous motion through a loss landscape. This captures optimization but does not expose the **arithmetic structure** underlying phase transitions, plateaus, and abrupt generalization events.

RTLG identifies the **signal-to-noise ratio** of a gradient system — a single positive real scalar — as the fundamental dynamical coordinate. By embedding this ratio into the positive cone of $SL(2,\mathbb{Z})$ via continued fraction approximation, every learning trajectory acquires a canonical discrete representation as a **word in two generators**, with continuous drift captured by hyperbolic flow in $SL(2,\mathbb{R})$.

The framework is built on established mathematics. The learning embedding is axiomatic and minimal. Connections to specific ML phenomena are stated as **conjectures**, not derivations.

**Notation.** $\mathbb{Z}$: integers. $\mathbb{Q}_{>0}$: positive rationals. $\mathbb{R}_{>0}$: positive reals. $SL(2,\mathbb{Z})$: $2\times 2$ integer matrices with determinant 1.

---

## 2. Algebraic Core: The Positive Monoid

### Definition 1 (Generators)

Define the matrices

$$L = \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}, \qquad R = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}.$$

Both lie in $SL(2,\mathbb{Z})$: entries are non-negative integers and $\det L = \det R = 1$.

### Definition 2 (Positive Monoid)

$$\mathcal{M} = \langle L, R \rangle \;\subset\; SL(2,\mathbb{Z})$$

is the monoid generated by $L$ and $R$ under matrix multiplication.

### Theorem 1 (Positive Cone Structure)

The monoid $\mathcal{M}$ equals the set of all matrices in $SL(2,\mathbb{Z})$ with non-negative integer entries:

$$\mathcal{M} \;=\; \Bigl\{\, M \in SL(2,\mathbb{Z}) \;\Big|\; M = \begin{pmatrix} a & b \\ c & d \end{pmatrix},\; a,b,c,d \geq 0 \,\Bigr\}.$$

Every element of $\mathcal{M}$ has a **unique** expression as a word in $\{L, R\}$.

**Proof sketch.**
- *Closure:* Products of non-negative-integer, determinant-1 matrices inherit both properties.
- *Containment:* Any $M$ with $a,b,c,d \geq 0$ and $\det M = 1$ reduces to the identity by the greedy Euclidean algorithm: right-multiply by $L^{-1}$ if $a \geq c$, by $R^{-1}$ if $c > a$, repeating until $M = I$. Entries strictly decrease so this terminates.
- *Uniqueness:* The algorithm is deterministic. No non-trivial cancellation exists among positive words because $L^{-1}$ and $R^{-1}$ have negative entries. $\square$

```python
import numpy as np

L = np.array([[1, 0], [1, 1]])
R = np.array([[1, 1], [0, 1]])
I = np.eye(2, dtype=int)

def matrix_to_word(M):
    """Recover the unique word in {L, R} for a positive-cone matrix."""
    M = np.array(M, dtype=int)
    L_inv = np.array([[1, 0], [-1, 1]])
    R_inv = np.array([[1, -1], [0,  1]])
    word = []
    for _ in range(200):
        if np.array_equal(M, I):
            break
        a, c = M[0, 0], M[1, 0]
        if a >= c:
            word.append('L');  M = M @ L_inv
        else:
            word.append('R');  M = M @ R_inv
    return ''.join(reversed(word))

# Verify: L @ L @ R encodes "LLR"
print(matrix_to_word(L @ L @ R))         # → LLR

# Verify closure: product of two positive-cone matrices stays positive-cone
A, B = L @ R @ L, R @ L @ R
C = A @ B
print(np.all(C >= 0) and int(round(np.linalg.det(C))) == 1)  # → True
```

---

## 3. Enumeration of Positive Rationals

### Definition 3 (Projective Map)

For $M = \begin{pmatrix}a&b\\c&d\end{pmatrix} \in \mathcal{M}$ with $c > 0$, define

$$\Phi(M) = \frac{a}{c} \;\in\; \mathbb{Q}_{>0}.$$

### Theorem 2 (Calkin–Wilf Correspondence)

$\Phi : \mathcal{M} \to \mathbb{Q}_{>0}$ is a bijection.

**Proof.**
- *Well-defined:* Since $\det M = ad - bc = 1$ and entries are non-negative integers, $\gcd(a,c) = 1$, so $a/c$ is already in lowest terms.
- *Injectivity:* Two matrices with the same ratio $a/c$ share the same Euclidean decomposition (since $\gcd(a,c)=1$ makes the algorithm deterministic), hence the same word in $\{L,R\}$, hence are equal.
- *Surjectivity:* Given $p/q \in \mathbb{Q}_{>0}$ in lowest terms, the Euclidean algorithm on $(p,q)$ yields partial quotients $[a_0; a_1, \ldots, a_k]$. Set $M = R^{a_0} L^{a_1} R^{a_2} \cdots$. Then $M \in \mathcal{M}$ and $\Phi(M) = p/q$. $\square$

```python
from fractions import Fraction
from math import gcd

def ratio_to_matrix(p, q):
    """Build the unique M in M with Phi(M) = p/q."""
    g = gcd(p, q);  p, q = p // g, q // g
    coeffs, a, b, toggle = [], p, q, 0
    while b:
        coeffs.append((a // b, toggle))
        a, b, toggle = b, a % b, toggle ^ 1
    M = I.copy()
    for k, t in reversed(coeffs):
        gen = R if t == 0 else L
        for _ in range(k):
            M = gen @ M
    return M

# Verify bijection
for p, q in [(3, 5), (7, 3), (1, 1), (5, 8)]:
    M  = ratio_to_matrix(p, q)
    ok = Fraction(M[0,0], M[1,0]) == Fraction(p, q)
    print(f"Phi(M({p}/{q})) == {p}/{q}? {ok}")   # → all True
```

---

## 4. Equivalence of Tree Structures

### 4.1 Calkin–Wilf Tree

The binary tree with root $1/1$ and child map

$$\frac{p}{q} \;\longmapsto\; \biggl(\frac{p}{p+q},\;\; \frac{p+q}{q}\biggr).$$

Each positive rational appears exactly once. The path from root to $p/q$ encodes the word in $\{L,R\}$ reaching $\Phi^{-1}(p/q)$.

### 4.2 Stern–Brocot Tree

Constructed by **mediant insertion**: given adjacent fractions $a/b$ and $c/d$, insert $(a+c)/(b+d)$. Starting from sentinels $0/1$ and $1/0$, every positive rational appears exactly once in magnitude order.

### Theorem 3 (Tree Isomorphism)

The Calkin–Wilf tree and the Stern–Brocot tree are isomorphic as rooted binary trees, differing only in their ordering of nodes (breadth-first by ancestry vs. in-order by magnitude).

**Proof.** Both realize the same continued fraction decomposition via the Euclidean algorithm. Calkin–Wilf labels by path from root; Stern–Brocot labels by in-order rank. $\square$

```python
def cw_children(p, q):
    return (p, p + q), (p + q, q)

def print_cw_tree(levels=3):
    nodes = [(1, 1)]
    for _ in range(levels):
        next_nodes = []
        for p, q in nodes:
            l, r = cw_children(p, q)
            print(f"  {p}/{q}  ->  {l[0]}/{l[1]},  {r[0]}/{r[1]}")
            next_nodes += [l, r]
        nodes = next_nodes

print_cw_tree(3)
```

### 4.3 Farey Adjacency

### Theorem 4 (Farey Determinant Condition)

Adjacent fractions in any Farey sequence $F_n$, and adjacent nodes along any edge of the Stern–Brocot tree, satisfy

$$ad - bc = 1.$$

**Proof.** The matrix $\begin{pmatrix}a&b\\c&d\end{pmatrix}$ formed by two Stern–Brocot neighbors lies in $SL(2,\mathbb{Z})$ by construction of the mediant. $\square$

### 4.4 Continued Fraction Tree

### Theorem 5 (Word–Continued Fraction Correspondence)

Let $p/q = [a_0;\, a_1,\, \ldots,\, a_k]$ with all $a_i \geq 1$ for $i \geq 1$ and $a_0 \geq 0$. The unique $M \in \mathcal{M}$ with $\Phi(M) = p/q$ is

$$M = R^{a_0}\, L^{a_1}\, R^{a_2}\, \cdots$$

**Corollary (Tree Depth).** The word length of $M$ equals $\displaystyle\sum_{i=0}^{k} a_i$.

**Proof.** Induction on the Euclidean algorithm. Each step $p \mapsto p - q$ corresponds to right-multiplying by $R^{-1}$; each step $q \mapsto q - p$ to $L^{-1}$. Counting steps recovers the partial quotients. $\square$

```python
def continued_fraction(p, q):
    coeffs = []
    while q:
        coeffs.append(p // q);  p, q = q, p % q
    return coeffs

def word_from_cf(coeffs):
    return ''.join(('R' if i % 2 == 0 else 'L') * a for i, a in enumerate(coeffs))

for p, q in [(5, 3), (8, 5), (7, 2)]:
    cf = continued_fraction(p, q)
    print(f"{p}/{q} = {cf}  word={word_from_cf(cf)}  depth={sum(cf)}")
# 5/3 = [1,1,2]  word=RLLL  depth=4
# 8/5 = [1,1,1,2]  word=RLLLL  depth=5   (Fibonacci → deepest)
# 7/2 = [3,2]    word=RRRLL  depth=5
```

### 4.5 Canonical Identification

| Structure | View |
|---|---|
| Calkin–Wilf tree | Combinatorial / ancestry order |
| Stern–Brocot tree | Combinatorial / magnitude order |
| Farey adjacency graph | Arithmetic / determinant condition |
| Continued fraction tree | Arithmetic / Euclidean algorithm |
| Positive cone of $SL(2,\mathbb{Z})$ | Algebraic / group |
| Discrete subgroup acting on $\mathbb{H}$ | Geometric |
| Word metric in generators $\{L,R\}$ | Metric |
| Exponentials of nilpotent Lie elements | Differential |

---

## 5. Hyperbolic Geometry Realization

### Theorem 6 (Hyperbolic Action)

$SL(2,\mathbb{R})$ acts on the upper half-plane $\mathbb{H} = \{z \in \mathbb{C} \mid \mathrm{Im}(z) > 0\}$ by Möbius transformations

$$M \cdot z = \frac{az + b}{cz + d}, \qquad M = \begin{pmatrix}a&b\\c&d\end{pmatrix},$$

and this action preserves the hyperbolic metric $ds^2 = (dx^2 + dy^2)/y^2$.

**Proof.** Standard; see Beardon (1983). Since $\det M = 1$ the Jacobian computation shows the metric tensor is invariant. $\square$

**Corollary.** $SL(2,\mathbb{Z})$ acts discretely by hyperbolic isometries. The rationals $\mathbb{Q} \cup \{\infty\}$ are exactly the cusps of this action. Tree paths in $\mathcal{M}$ correspond to geodesic rays in $\mathbb{H}$ terminating at rational boundary points.

```python
import cmath

def mobius(M, z):
    a, b, c, d = M[0,0], M[0,1], M[1,0], M[1,1]
    return (a * z + b) / (c * z + d)

# Geodesic ray toward the boundary: repeated R-steps drive Im(z) → 0
z = complex(0, 1)
for step in range(6):
    z = mobius(R, z)
    print(f"Step {step+1}: Re={z.real:.4f}  Im={z.imag:.6f}")
# Im shrinks toward 0, approaching the rational boundary
```

---

## 6. Lie Algebra Structure and Continuous Flow

### Definition 4 (Standard Basis of $\mathfrak{sl}(2,\mathbb{R})$)

$$H = \begin{pmatrix}1&0\\0&-1\end{pmatrix}, \qquad E = \begin{pmatrix}0&1\\0&0\end{pmatrix}, \qquad F = \begin{pmatrix}0&0\\1&0\end{pmatrix}.$$

These satisfy the bracket relations $[H,E]=2E$, $[H,F]=-2F$, $[E,F]=H$.

### Theorem 7 (Nilpotent Exponentials)

$$\exp(E) = R, \qquad \exp(F) = L.$$

**Proof.** Since $E^2 = 0$, the series truncates: $\exp(E) = I + E = \begin{pmatrix}1&1\\0&1\end{pmatrix} = R$. Identical for $F$. $\square$

```python
from scipy.linalg import expm

E = np.array([[0., 1.], [0., 0.]])
F = np.array([[0., 0.], [1., 0.]])
H = np.array([[1., 0.], [0., -1.]])

print("exp(E):\n", np.round(expm(E)).astype(int))   # → R
print("exp(F):\n", np.round(expm(F)).astype(int))   # → L

# Lie bracket [H, E] == 2E
print(np.allclose(H @ E - E @ H, 2 * E))   # → True
```

### Theorem 8 (Hyperbolic Flow)

$$\exp(tH) = \begin{pmatrix}e^t&0\\0&e^{-t}\end{pmatrix}.$$

Under the projective map $\Phi$, this induces $\Phi\bigl(\exp(tH) \cdot M\bigr) = e^{2t}\,\Phi(M)$, so $\log\Phi(M(t))$ evolves **linearly** at rate $2\lambda$ under $\dot{M} = \lambda H \cdot M$.

**Proof.** Direct: $\exp(tH) \cdot M$ scales the first row by $e^t$ and the second by $e^{-t}$, so $a/c \mapsto e^{2t}(a/c)$. $\square$

```python
def hyperbolic_flow(M, t):
    scale = np.array([[np.exp(t), 0], [0, np.exp(-t)]])
    return scale @ M

M0 = R @ L @ R
t  = 1.5
Mt = hyperbolic_flow(M0.astype(float), t)

phi0 = M0[0, 0] / M0[1, 0]
phit = Mt[0, 0] / Mt[1, 0]
print(f"e^(2t)*Phi(M0) = {np.exp(2*t)*phi0:.6f}")
print(f"Phi(exp(tH)M0) = {phit:.6f}")   # → equal
```

---

## 7. Diophantine Approximation and Depth

### Theorem 10 (Convergent Error Bound)

Let $x \in \mathbb{R}_{>0}$ and $p_n/q_n$ be the $n$-th continued fraction convergent of $x$. Then

$$\Bigl|\, x - \frac{p_n}{q_n} \,\Bigr| < \frac{1}{q_n^2}.$$

**Proof.** Classical. Follows from the recurrence $p_n = a_n p_{n-1} + p_{n-2}$ and the identity $p_n q_{n-1} - p_{n-1} q_n = (-1)^{n-1}$. $\square$

**Corollary (Depth–Precision Relation).** Approximation error at depth $d = \sum a_i$ is $O(1/q^2)$ where $q$ grows at least as fast as the Fibonacci sequence. Deeper nodes represent arithmetically more complex, higher-precision states.

```python
from math import floor

def convergents(x, n_terms=8):
    """Return continued fraction convergents p_n/q_n for x."""
    a      = floor(x);  frac = x - a;  coeffs = [a]
    for _ in range(n_terms - 1):
        if frac == 0: break
        x = 1 / frac;  a = floor(x);  frac = x - a;  coeffs.append(a)
    p_prev, p_curr = 1, coeffs[0]
    q_prev, q_curr = 0, 1
    results = [(p_curr, q_curr)]
    for a in coeffs[1:]:
        p_prev, p_curr = p_curr, a * p_curr + p_prev
        q_prev, q_curr = q_curr, a * q_curr + q_prev
        results.append((p_curr, q_curr))
    return results

import math
x = math.pi
for p, q in convergents(x):
    err   = abs(x - p/q)
    bound = 1 / q**2
    print(f"  {p}/{q:<6}  error={err:.2e}  1/q²={bound:.2e}  ok={err < bound}")
```

---

## 8. Stochastic Extension

When gradient statistics evolve stochastically, the ratio $C(t) = S(t)/N(t)$ is a continuous-time positive random process. The natural lift to $SL(2,\mathbb{R})$ is the left-invariant SDE:

$$dM_t = M_t\Bigl(\lambda H\,dt + \sigma_E\,dW^E_t\,E + \sigma_F\,dW^F_t\,F\Bigr).$$

**Proposition.** By Itô's lemma applied to $\theta(t) = \log C(t)$:

$$d\theta_t = \Bigl(2\lambda - \tfrac{1}{2}(\sigma_E^2 + \sigma_F^2)\Bigr)\,dt \;+\; \sigma_E\,dW^E_t \;+\; \sigma_F\,dW^F_t.$$

This is a one-dimensional Brownian motion with drift; first-passage time distributions to any threshold follow from the reflection principle.

*This SDE is a modeling choice. Its validity for any particular gradient system is an empirical question.*

```python
def simulate_log_snr(lam, sigma_E, sigma_F, T=500, dt=0.01, seed=42):
    """Simulate log C(t) under the stochastic model."""
    rng   = np.random.default_rng(seed)
    drift = 2 * lam - 0.5 * (sigma_E**2 + sigma_F**2)
    sigma = np.sqrt(sigma_E**2 + sigma_F**2)
    steps = int(T / dt)
    theta = np.zeros(steps)
    for i in range(1, steps):
        theta[i] = theta[i-1] + drift*dt + sigma*np.sqrt(dt)*rng.standard_normal()
    return theta

theta = simulate_log_snr(lam=0.1, sigma_E=0.3, sigma_F=0.3)
print(f"Final log-SNR: {theta[-1]:.3f}  (expected drift ≈ {2*0.1*500:.0f})")
```

---

## 9. The Learning Embedding

### Definition 5 (Gradient Statistics)

Let $g_t \in \mathbb{R}^d$ be a stochastic gradient decomposed as $g_t = s_t + \xi_t$, where $s_t = \mathbb{E}[g_t]$ and $\xi_t$ is zero-mean noise. Define:

$$S_t := \|s_t\|, \qquad N_t := \sqrt{\mathbb{E}\|\xi_t\|^2}, \qquad C(t) = S_t / N_t.$$

### Axiom 1 (Learning Embedding)

At each time $t$, the learning state is represented by the unique $M_t \in \mathcal{M}$ satisfying $\Phi(M_t) = p_t/q_t$, where $p_t/q_t$ is the best rational convergent of $C(t)$ at a fixed precision level.

*This is an embedding, not an identification. The gradient system does not become a matrix group; its scalar ratio is approximated by a rational point in the tree.*

### Axiom 2 (Binary Dominance)

At each discrete step: $\Delta\log C_t = \log C_{t+1} - \log C_t \neq 0$ (generically).

### Theorem 11 (Learning Trajectory Embedding)

Under Axioms 1 and 2, every gradient system induces a unique path in $\mathcal{M}$:

$$M_{t+1} = G_t\, M_t, \quad G_t \in \{L, R\},$$

where $G_t = R$ if $\Delta\log C_t > 0$, and $G_t = L$ if $\Delta\log C_t < 0$.

**Proof.** Each $C(t)$ maps to a unique node by Axiom 1 and Theorem 2. Each sign determines a unique generator by Axiom 2. Words in $\{L,R\}$ uniquely determine elements of $\mathcal{M}$ by Theorem 1. $\square$

```python
def embed_trajectory(snr_sequence, precision=6):
    """Embed a sequence of SNR values into a path in M."""
    word = []
    for i in range(1, len(snr_sequence)):
        delta = np.log(snr_sequence[i]) - np.log(snr_sequence[i-1])
        if   delta > 0: word.append('R')
        elif delta < 0: word.append('L')
    return ''.join(word)

# Synthetic trace: gradual rise, plateau, sudden jump
snr_trace = (
    [0.5 + 0.02*i for i in range(20)] +
    [1.4] * 30                         +
    [1.4 + 0.1*i for i in range(10)]
)
word = embed_trajectory(snr_trace)
print(f"Word (first 20): {word[:20]}")
print(f"Depth: {len(word)}")
```

---

## 10. Derived Learning Invariants

**Tree Depth $d(t)$** — the word length of $M_t$ in $\{L,R\}$, equal to $\sum_i a_i$. Measures the arithmetic complexity of the current learning state.

**Path Entropy $\mathcal{H}(t)$** — the Shannon entropy of the $L/R$ string up to time $t$:

$$\mathcal{H}(t) = -\hat{p}_L \log_2 \hat{p}_L - \hat{p}_R \log_2 \hat{p}_R.$$

$\mathcal{H}(t) \approx 0$ indicates persistent signal or noise dominance; $\mathcal{H}(t) \approx 1$ indicates oscillation.

**Approximation Residual $\varepsilon(t)$** — the gap $|C(t) - p_t/q_t| < 1/q_t^2$. Large $\varepsilon$ indicates the state lies between two tree nodes.

```python
from math import log2

def compute_invariants(word_string, snr_sequence, precision=6):
    depths, entropies, residuals = [], [], []
    for t in range(1, len(word_string) + 1):
        sub = word_string[:t]
        depths.append(t)
        pL = sub.count('L') / len(sub);  pR = 1 - pL
        h  = -(pL*log2(pL) if pL > 0 else 0) - (pR*log2(pR) if pR > 0 else 0)
        entropies.append(h)
    for snr in snr_sequence:
        p, q = convergents(snr, n_terms=precision)[-1]
        residuals.append(abs(snr - p / q))
    return depths, entropies, residuals

depths, entropies, residuals = compute_invariants(word, snr_trace)
print(f"Final entropy:  {entropies[-1]:.4f}")
print(f"Mean residual:  {sum(residuals)/len(residuals):.6f}")
```

---

## 11. Conjectured Correspondences in ML

The following are **proposed research directions**, not proven results.

| Phenomenon | Proposed RTLG Interpretation | Falsifiable Prediction |
|---|---|---|
| **Grokking** | Path traverses a high-depth region then collapses to a simpler subtree. | $d(t)$ spikes then sharply drops at the grokking step; $\mathcal{H}(t) \approx 0$ during plateau. |
| **Double Descent** | $C(t)$ oscillates near $1$, causing the path to revisit the tree root region. | $C(t) \approx 1$ at interpolation threshold; $d(t)$ is locally maximal there. |
| **Flat Minima** | Low-depth nodes are more stable under perturbation. | Flat minima correlate with low $d(t)$ and small denominator $q_t$. |
| **Lottery Tickets** | Pruning selects the simplest ancestor node consistent with surviving parameters. | Pruned networks have lower $d(t)$ at equivalent performance. |
| **Adaptive Optimizers** | Adam implicitly runs independent tree embeddings per coordinate. | Per-parameter $C_i(t)$ satisfies the embedding theorem coordinate-wise. |

Testing requires measuring $S_t$ and $N_t$ during training, feasible with standard gradient logging.

---

## 12. Canonical Summary

The most compact statement of RTLG is:

> The space of positive real signal–noise ratios admits a canonical embedding into the positive monoid of $SL(2,\mathbb{Z})$ via continued fraction approximation. This monoid is equivalently described by the Calkin–Wilf tree, the Stern–Brocot tree, Farey adjacency, and continued fraction expansion — all unified as the rational boundary of the hyperbolic plane under the standard $SL(2,\mathbb{R})$ action. Discrete learning steps correspond to multiplication by generators $L$ and $R$, which are exponentials of nilpotent Lie algebra elements; continuous signal drift corresponds to the diagonal hyperbolic flow generated by $H$.

The framework reduces to a single algebraic object:

$$\text{Positive cone of } SL(2,\mathbb{Z})$$

All combinatorial, arithmetic, geometric, and differential structures are equivalent presentations of this object.

---

## 13. Open Questions

**Irrational and periodic trajectories.** Quadratic irrationals have eventually periodic continued fraction expansions. Do learning systems with cyclic learning rate schedules produce periodic paths in $\mathcal{M}$?

**$p$-adic interpretation.** The infinite path string $G_0 G_1 G_2 \cdots \in \{L,R\}^{\mathbb{N}}$ defines a boundary point of the tree, analogous to a 2-adic integer. Can $p$-adic analysis provide useful metrics on learning trajectory space?

**Vector RTLG.** For networks with $d$ parameters, one obtains $d$ independent ratios $C_i(t)$, each inducing a path in $\mathcal{M}$. The joint state lives in $\mathcal{M}^d$. What algebraic and geometric structure governs this product space?

**Minkowski's question-mark function.** The function $\mathcal{Q}(x) : [0,\infty) \to [0,1]$ is a strictly increasing homeomorphism with derivative zero almost everywhere. Setting $\ell(t) = \mathcal{Q}(C(t))$ linearizes the learning coordinate: plateaus correspond to flat regions of $\mathcal{Q}$, genuine learning events to its jumps. The relationship between $\Delta\ell(t)$ and observable generalization transitions warrants systematic experimental investigation.

**Empirical validation.** The primary open question is whether the embedding has predictive power beyond structural visualization. This requires controlled experiments comparing tree-depth and path-entropy trajectories against known phase transitions across architectures and training regimes.

---

> **Every learning trajectory is a word. Every word is a rational. Every rational is a path through hyperbolic space — and now you can read them.**

